{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vengie/Project42_ELOGISTIX/blob/main/OOPS%20ML%20WF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DnKI2CXMScbW"
      },
      "outputs": [],
      "source": [
        "from numpy import median\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.cleaned_data = None\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
        "\n",
        "    def clean_data(self):\n",
        "        # Dropping rows with missing values in the target column\n",
        "        self.cleaned_data = self.data.dropna(subset=['Reached.on.Time_Y.N'])\n",
        "\n",
        "        # Convert categorical columns to numerical representations\n",
        "        label_encoder = LabelEncoder()\n",
        "        categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            self.cleaned_data[col] = label_encoder.fit_transform(self.cleaned_data[col])\n",
        "\n",
        "        # Ensure 'Reached.on.Time_Y.N' is numeric (if it's not already)\n",
        "        self.cleaned_data['Reached.on.Time_Y.N'] = pd.to_numeric(self.cleaned_data['Reached.on.Time_Y.N'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "        # Handle outliers\n",
        "        self.cleaned_data['Weight_in_gms'] = self._handle_outliers(self.cleaned_data['Weight_in_gms'])\n",
        "\n",
        "        # Handling missing values in other columns using SimpleImputer\n",
        "        numerical_cols = ['Customer_care_calls', 'Customer_rating', 'Prior_purchases', 'Weight_in_gms']\n",
        "        categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "        imputer_num = SimpleImputer(strategy='median')\n",
        "        self.cleaned_data[numerical_cols] = imputer_num.fit_transform(self.cleaned_data[numerical_cols])\n",
        "\n",
        "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "        self.cleaned_data[categorical_cols] = imputer_cat.fit_transform(self.cleaned_data[categorical_cols])\n",
        "\n",
        "        # Check for any remaining missing values\n",
        "        if self.cleaned_data.isnull().values.any():\n",
        "            raise ValueError(\"There are still missing values in the dataset after preprocessing.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _handle_outliers(series):\n",
        "        # Find the median and interquartile range\n",
        "        median_val = series.median()\n",
        "        IQR = series.quantile(0.75) - series.quantile(0.25)\n",
        "\n",
        "        # Define limits for outliers\n",
        "        lower_limit = series.quantile(0.25) - 1.5 * IQR\n",
        "        upper_limit = series.quantile(0.75) + 1.5 * IQR\n",
        "\n",
        "        # Replace outliers with the closest limit\n",
        "        series = series.apply(lambda x: upper_limit if x > upper_limit else (lower_limit if x < lower_limit else x))\n",
        "        return series\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        if self.cleaned_data is not None:\n",
        "            try:\n",
        "                initial_columns = self.cleaned_data.columns.tolist()\n",
        "\n",
        "                # Mapping categorical variables to numerical values\n",
        "                self.cleaned_data['Warehouse_block'] = self.cleaned_data['Warehouse_block'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'F': 4})\n",
        "                self.cleaned_data['Mode_of_Shipment'] = self.cleaned_data['Mode_of_Shipment'].map({'Flight': 0, 'Ship': 1})\n",
        "\n",
        "                # Creating a new feature based on 'Cost_of_the_Product' and 'Discount_offered'\n",
        "                self.cleaned_data['Discounted_Cost'] = self.cleaned_data['Cost_of_the_Product'] - (self.cleaned_data['Cost_of_the_Product'] * self.cleaned_data['Discount_offered'] / 100)\n",
        "\n",
        "                # Encoding categorical variables\n",
        "                encoded_cols = pd.get_dummies(self.cleaned_data[['Product_importance', 'Gender']])\n",
        "                self.cleaned_data = pd.concat([self.cleaned_data, encoded_cols], axis=1)  # Concatenate the encoded columns\n",
        "\n",
        "                # Dropping irrelevant or transformed columns\n",
        "                self.cleaned_data.drop(['Cost_of_the_Product', 'Discount_offered', 'Product_importance', 'Gender'], axis=1, inplace=True)\n",
        "\n",
        "                final_columns = self.cleaned_data.columns.tolist()\n",
        "                print(\"Initial columns:\", initial_columns)\n",
        "                print(\"Final columns:\", final_columns)\n",
        "\n",
        "                self.cleaned_data.reset_index(drop=True, inplace=True)  # Resetting indices\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in feature engineering: {e}\")\n",
        "                raise ValueError(\"An error occurred during feature engineering.\")\n",
        "        else:\n",
        "            raise ValueError(\"No data available. Please run the 'clean_data' method first.\")\n",
        "\n",
        "\n",
        "    def handle_missing_values(self):\n",
        "        if self.cleaned_data is not None:\n",
        "            try:\n",
        "                imputer = SimpleImputer(strategy='mean')\n",
        "                imputed_data = imputer.fit_transform(self.cleaned_data)\n",
        "\n",
        "                # Ensure the columns are aligned with the original DataFrame\n",
        "                imputed_df = pd.DataFrame(imputed_data, columns=self.cleaned_data.columns)\n",
        "\n",
        "                self.cleaned_data = imputed_df  # Replace the entire cleaned_data with the imputed DataFrame\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in handling missing values: {e}\")\n",
        "                raise ValueError(\"An error occurred while handling missing values.\")\n",
        "        else:\n",
        "            raise ValueError(\"No data available. Please run the 'clean_data' method first.\")\n",
        "\n",
        "\n",
        "    def split_data(self, target_column, test_size=0.2, random_state=42):\n",
        "        if self.cleaned_data is None:\n",
        "            raise ValueError(\"Data hasn't been cleaned yet. Please run 'clean_data' method first.\")\n",
        "\n",
        "        features = self.cleaned_data.drop(columns=[target_column])\n",
        "        target = self.cleaned_data[target_column]\n",
        "\n",
        "        # Splitting the data into training and testing sets\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(features, target, test_size=test_size, random_state=random_state)\n",
        "\n",
        "\n",
        "\n",
        "    def perform_preprocessing(self, target_column):\n",
        "        self.clean_data()\n",
        "        self.feature_engineering()\n",
        "        # self.handle_missing_values()\n",
        "        self.split_data(target_column)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nDoSD1v0S1iq"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     # Load your dataset into 'data'\n",
        "#     data_source='https://raw.githubusercontent.com/vengie/Project42_ELOGISTIX/main/Data/DSMMProject42-CPL-5559-Ecom_Shipping_stride.csv'\n",
        "#     data = pd.read_csv(data_source)\n",
        "\n",
        "#     # Create an instance of DataPreprocessor\n",
        "#     preprocessor = DataPreprocessor(data)\n",
        "\n",
        "#     # Perform preprocessing (specify the target column name)\n",
        "#     target_column = 'Reached.on.Time_Y.N'\n",
        "#     preprocessor.perform_preprocessing(target_column)\n",
        "\n",
        "#     # Access the processed data\n",
        "#     X_train = preprocessor.X_train\n",
        "#     X_test = preprocessor.X_test\n",
        "#     y_train = preprocessor.y_train\n",
        "#     y_test = preprocessor.y_test\n",
        "\n",
        "# Now you can use X_train, X_test, y_train, y_test for training your models\n",
        "# For example:\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "# ... (continue with model training and evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQMiJu1TYNTw"
      },
      "source": [
        "Model Selection and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zsUf83BsS6TB"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "# class ModelTrainer:\n",
        "#     def __init__(self, data_preprocessor):\n",
        "#         self.data_preprocessor = data_preprocessor\n",
        "#         self.models = {\n",
        "#             'Logistic Regression': LogisticRegression(),\n",
        "#             'Decision Tree': DecisionTreeClassifier(),\n",
        "#             'XGBoost': XGBClassifier(),\n",
        "#             'SVM': SVC()\n",
        "#         }\n",
        "#         self.trained_models = {}\n",
        "\n",
        "#     def train_models(self):\n",
        "#         for model_name, model in self.models.items():\n",
        "#             X_train = self.data_preprocessor.train_data.drop(columns=['Reached.on.Time_Y.N'])\n",
        "#             y_train = self.data_preprocessor.train_data['Reached.on.Time_Y.N']\n",
        "#             model.fit(X_train, y_train)\n",
        "#             self.trained_models[model_name] = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "swr1Z95ZYcYy"
      },
      "outputs": [],
      "source": [
        "#model_trainer.py\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import SimpleImputer\n",
        "# from model_evaluator import ModelEvaluator  # Importing ModelEvaluator from model_evaluator.py\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(),\n",
        "            'Decision Tree': DecisionTreeClassifier(),\n",
        "            'XGBoost': XGBClassifier(),\n",
        "            'SVM': SVC()\n",
        "        }\n",
        "\n",
        "    def train_models(self):\n",
        "        # Impute missing values in X_train if any\n",
        "        imputer = SimpleImputer(strategy='mean')  # Or use strategy='median' or 'most_frequent' as needed\n",
        "        self.X_train = imputer.fit_transform(self.X_train)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "            self.models[name] = model\n",
        "\n",
        "    def predict(self):\n",
        "        predictions = {}\n",
        "        for name, model in self.models.items():\n",
        "            # Impute missing values in X_test if any\n",
        "            imputer = SimpleImputer(strategy='mean')  # Or use strategy='median' or 'most_frequent' as needed\n",
        "            X_test_imputed = imputer.fit_transform(self.X_test)\n",
        "\n",
        "            print(f\"Making predictions using {name}...\")\n",
        "            y_pred = model.predict(X_test_imputed)\n",
        "            predictions[name] = y_pred\n",
        "\n",
        "            # Evaluate model\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            report = classification_report(self.y_test, y_pred)\n",
        "\n",
        "            print(f\"{name} Accuracy: {accuracy}\")\n",
        "            print(f\"{name} Report: \\n{report}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        # Impute missing values in X_test if any\n",
        "        imputer = SimpleImputer(strategy='mean')  # Or use strategy='median' or 'most_frequent' as needed\n",
        "        self.X_test = imputer.fit_transform(self.X_test)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"Evaluating {name}...\")\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            # Evaluate model\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            report = classification_report(self.y_test, y_pred)\n",
        "\n",
        "            print(f\"{name} Accuracy: {accuracy}\")\n",
        "            print(f\"{name} Report: \\n{report}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "19ufcBOjf8RC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # model_evaluation.py\n",
        "\n",
        "# class ModelEvaluator:\n",
        "#     def __init__(self, X_test, y_test):\n",
        "#         self.X_test = X_test\n",
        "#         self.y_test = y_test\n",
        "\n",
        "#     def evaluate_model(self, model, name):\n",
        "#         print(f\"Evaluating {name}...\")\n",
        "#         y_pred = model.predict(self.X_test)\n",
        "\n",
        "#         # Evaluate model\n",
        "#         accuracy = accuracy_score(self.y_test, y_pred)\n",
        "#         report = classification_report(self.y_test, y_pred)\n",
        "\n",
        "#         print(f\"{name} Accuracy: {accuracy}\")\n",
        "#         print(f\"{name} Report: \\n{report}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7GkQV606cM2t"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters for each model\n",
        "param_grid = {\n",
        "    'Logistic Regression': {'C': [0.1, 1.0, 10.0], 'solver': ['liblinear', 'lbfgs']},\n",
        "    'Decision Tree': {'max_depth': [None, 5, 10, 15], 'min_samples_split': [2, 5, 10]},\n",
        "    'XGBoost': {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 150]},\n",
        "    'SVM': {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf']}\n",
        "}\n",
        "\n",
        "class ModelTrainerWithHyperparamTuning:\n",
        "    def __init__(self, X_train, X_test, y_train, y_test):\n",
        "        # Same as before\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "            'Decision Tree': DecisionTreeClassifier(),\n",
        "            'XGBoost': XGBClassifier(),\n",
        "            'SVM': SVC()\n",
        "        }\n",
        "\n",
        "    def handle_missing_values(self):\n",
        "        # Create an imputer and fit it to the training data\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        imputer.fit(self.X_train)\n",
        "\n",
        "        # Transform both training and test data\n",
        "        self.X_train = imputer.transform(self.X_train)\n",
        "        self.X_test = imputer.transform(self.X_test)\n",
        "\n",
        "    def train_models_with_hyperparam_tuning(self):\n",
        "        self.handle_missing_values()  # Handle missing values before training models\n",
        "        for name, model in self.models.items():\n",
        "            if name == 'Logistic Regression':\n",
        "                model.set_params(max_iter=1000)  # Increase max_iter for Logistic Regression\n",
        "\n",
        "            print(f\"Tuning hyperparameters for {name}...\")\n",
        "            param_grid_model = param_grid[name]\n",
        "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid_model, scoring='accuracy', cv=5)\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            best_params = grid_search.best_params_\n",
        "            print(f\"Best hyperparameters for {name}: {best_params}\")\n",
        "\n",
        "            # Set the model with the best hyperparameters\n",
        "            self.models[name] = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "    def perform_evaluation(self, X_test, y_test):\n",
        "        evaluation_results = {}\n",
        "        for name, model in self.models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred)\n",
        "            recall = recall_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "            evaluation_results[name] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1 Score': f1\n",
        "            }\n",
        "\n",
        "        return evaluation_results\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Existing code to load data, preprocess, and get X_train, X_test, y_train, y_test\n",
        "\n",
        "#     trainer_with_hyperparam_tuning = ModelTrainerWithHyperparamTuning(X_train, X_test, y_train, y_test)\n",
        "#     trainer_with_hyperparam_tuning.train_models_with_hyperparam_tuning()\n",
        "#     trainer_with_hyperparam_tuning.evaluate_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0gwJc5M6Zqjt"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import time\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # data_source = 'https://raw.githubusercontent.com/vengie/Project42_ELOGISTIX/main/Data/DSMMProject42-CPL-5559-Ecom_Shipping_stride.csv'\n",
        "#     # data = pd.read_csv(data_source)\n",
        "\n",
        "#     # preprocessor = DataPreprocessor(data)\n",
        "#     # target_column = 'Reached.on.Time_Y.N'\n",
        "#     # preprocessor.perform_preprocessing(target_column)\n",
        "\n",
        "#     # X_train = preprocessor.X_train\n",
        "#     # X_test = preprocessor.X_test\n",
        "#     # y_train = preprocessor.y_train\n",
        "#     # y_test = preprocessor.y_test\n",
        "\n",
        "#     # trainer = ModelTrainer(X_train, X_test, y_train, y_test)\n",
        "#     # trainer.train_models()\n",
        "#     # trainer.evaluate_models()\n",
        "\n",
        "#     # # Instantiate ModelTrainerWithHyperparamTuning class and train models\n",
        "#     # trainer_with_hyperparam_tuning = ModelTrainerWithHyperparamTuning(X_train, X_test, y_train, y_test)\n",
        "#     # trainer_with_hyperparam_tuning.train_models_with_hyperparam_tuning()\n",
        "#     # # Handle missing values in the test data\n",
        "#     # imputer = SimpleImputer(strategy='mean')\n",
        "#     # X_test_imputed = imputer.fit_transform(X_test)\n",
        "#     # # Evaluate models using the cleaned test data\n",
        "#     # evaluation_results = trainer_with_hyperparam_tuning.perform_evaluation(X_test_imputed, y_test)\n",
        "\n",
        "#     start_total_time = time.time()\n",
        "\n",
        "#     data_source = 'https://raw.githubusercontent.com/vengie/Project42_ELOGISTIX/main/Data/DSMMProject42-CPL-5559-Ecom_Shipping_stride.csv'\n",
        "#     data = pd.read_csv(data_source)\n",
        "\n",
        "#     start_preprocessing_time = time.time()\n",
        "#     preprocessor = DataPreprocessor(data)\n",
        "#     target_column = 'Reached.on.Time_Y.N'\n",
        "#     preprocessor.perform_preprocessing(target_column)\n",
        "#     end_preprocessing_time = time.time()\n",
        "\n",
        "#     X_train = preprocessor.X_train\n",
        "#     X_test = preprocessor.X_test\n",
        "#     y_train = preprocessor.y_train\n",
        "#     y_test = preprocessor.y_test\n",
        "\n",
        "#     start_trainer_time = time.time()\n",
        "#     trainer = ModelTrainer(X_train, X_test, y_train, y_test)\n",
        "#     trainer.train_models()\n",
        "#     end_trainer_time = time.time()\n",
        "#     trainer.evaluate_models()\n",
        "\n",
        "#     start_hyperparam_trainer_time = time.time()\n",
        "#     trainer_with_hyperparam_tuning = ModelTrainerWithHyperparamTuning(X_train, X_test, y_train, y_test)\n",
        "#     trainer_with_hyperparam_tuning.train_models_with_hyperparam_tuning()\n",
        "#     end_hyperparam_trainer_time = time.time()\n",
        "\n",
        "#     imputer = SimpleImputer(strategy='mean')\n",
        "#     start_imputer_time = time.time()\n",
        "#     X_test_imputed = imputer.fit_transform(X_test)\n",
        "#     end_imputer_time = time.time()\n",
        "\n",
        "#     start_evaluation_time = time.time()\n",
        "#     evaluation_results = trainer_with_hyperparam_tuning.perform_evaluation(X_test_imputed, y_test)\n",
        "#     end_evaluation_time = time.time()\n",
        "\n",
        "#     total_time = end_total_time - start_total_time\n",
        "#     preprocessing_time = end_preprocessing_time - start_preprocessing_time\n",
        "#     trainer_time = end_trainer_time - start_trainer_time\n",
        "#     hyperparam_trainer_time = end_hyperparam_trainer_time - start_hyperparam_trainer_time\n",
        "#     imputer_time = end_imputer_time - start_imputer_time\n",
        "#     evaluation_time = end_evaluation_time - start_evaluation_time\n",
        "\n",
        "#     print(f\"Total execution time: {total_time} seconds\")\n",
        "#     print(f\"Preprocessing time: {preprocessing_time} seconds\")\n",
        "#     print(f\"Trainer time: {trainer_time} seconds\")\n",
        "#     print(f\"Hyperparameter trainer time: {hyperparam_trainer_time} seconds\")\n",
        "#     print(f\"Imputer time: {imputer_time} seconds\")\n",
        "#     print(f\"Evaluation time: {evaluation_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj21xKEOycml",
        "outputId": "4d5ef431-e897-4d3f-b65a-b5fd23081e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial columns: ['ID', 'Warehouse_block', 'Mode_of_Shipment', 'Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product', 'Prior_purchases', 'Product_importance', 'Gender', 'Discount_offered', 'Weight_in_gms', 'Reached.on.Time_Y.N']\n",
            "Final columns: ['ID', 'Warehouse_block', 'Mode_of_Shipment', 'Customer_care_calls', 'Customer_rating', 'Prior_purchases', 'Weight_in_gms', 'Reached.on.Time_Y.N', 'Discounted_Cost']\n",
            "Training Logistic Regression...\n",
            "Training Decision Tree...\n",
            "Training XGBoost...\n",
            "Training SVM...\n",
            "Evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.6504545454545455\n",
            "Logistic Regression Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.53      0.55       895\n",
            "           1       0.70      0.73      0.71      1305\n",
            "\n",
            "    accuracy                           0.65      2200\n",
            "   macro avg       0.64      0.63      0.63      2200\n",
            "weighted avg       0.65      0.65      0.65      2200\n",
            "\n",
            "Evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.6618181818181819\n",
            "Decision Tree Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.57      0.58       895\n",
            "           1       0.71      0.72      0.72      1305\n",
            "\n",
            "    accuracy                           0.66      2200\n",
            "   macro avg       0.65      0.65      0.65      2200\n",
            "weighted avg       0.66      0.66      0.66      2200\n",
            "\n",
            "Evaluating XGBoost...\n",
            "XGBoost Accuracy: 0.6554545454545454\n",
            "XGBoost Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.64      0.60       895\n",
            "           1       0.73      0.66      0.70      1305\n",
            "\n",
            "    accuracy                           0.66      2200\n",
            "   macro avg       0.65      0.65      0.65      2200\n",
            "weighted avg       0.66      0.66      0.66      2200\n",
            "\n",
            "Evaluating SVM...\n",
            "SVM Accuracy: 0.6922727272727273\n",
            "SVM Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.99      0.72       895\n",
            "           1       0.98      0.49      0.65      1305\n",
            "\n",
            "    accuracy                           0.69      2200\n",
            "   macro avg       0.78      0.74      0.69      2200\n",
            "weighted avg       0.81      0.69      0.68      2200\n",
            "\n",
            "Tuning hyperparameters for Logistic Regression...\n",
            "Best hyperparameters for Logistic Regression: {'C': 0.1, 'solver': 'liblinear'}\n",
            "Tuning hyperparameters for Decision Tree...\n",
            "Best hyperparameters for Decision Tree: {'max_depth': 5, 'min_samples_split': 2}\n",
            "Tuning hyperparameters for XGBoost...\n",
            "Best hyperparameters for XGBoost: {'max_depth': 3, 'n_estimators': 50}\n",
            "Tuning hyperparameters for SVM...\n",
            "Best hyperparameters for SVM: {'C': 1.0, 'kernel': 'rbf'}\n",
            "Making predictions using Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.6504545454545455\n",
            "Logistic Regression Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.53      0.55       895\n",
            "           1       0.70      0.73      0.71      1305\n",
            "\n",
            "    accuracy                           0.65      2200\n",
            "   macro avg       0.64      0.63      0.63      2200\n",
            "weighted avg       0.65      0.65      0.65      2200\n",
            "\n",
            "Making predictions using Decision Tree...\n",
            "Decision Tree Accuracy: 0.6618181818181819\n",
            "Decision Tree Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.57      0.58       895\n",
            "           1       0.71      0.72      0.72      1305\n",
            "\n",
            "    accuracy                           0.66      2200\n",
            "   macro avg       0.65      0.65      0.65      2200\n",
            "weighted avg       0.66      0.66      0.66      2200\n",
            "\n",
            "Making predictions using XGBoost...\n",
            "XGBoost Accuracy: 0.6554545454545454\n",
            "XGBoost Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.64      0.60       895\n",
            "           1       0.73      0.66      0.70      1305\n",
            "\n",
            "    accuracy                           0.66      2200\n",
            "   macro avg       0.65      0.65      0.65      2200\n",
            "weighted avg       0.66      0.66      0.66      2200\n",
            "\n",
            "Making predictions using SVM...\n",
            "SVM Accuracy: 0.6922727272727273\n",
            "SVM Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.99      0.72       895\n",
            "           1       0.98      0.49      0.65      1305\n",
            "\n",
            "    accuracy                           0.69      2200\n",
            "   macro avg       0.78      0.74      0.69      2200\n",
            "weighted avg       0.81      0.69      0.68      2200\n",
            "\n",
            "Total execution time: 5763.54616355896 seconds\n",
            "Preprocessing time: 0.058448076248168945 seconds\n",
            "Trainer time: 5.410754680633545 seconds\n",
            "Hyperparameter trainer time: 5752.289888620377 seconds\n",
            "Predict time: 1.5245261192321777 seconds\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from ModelPreprocessor import DataPreprocessor\n",
        "# from ModelTrainer import ModelTrainer, ModelTrainerWithHyperparamTuning\n",
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_total_time = time.time()\n",
        "\n",
        "    data_source = 'https://raw.githubusercontent.com/vengie/Project42_ELOGISTIX/main/Data/DSMMProject42-CPL-5559-Ecom_Shipping_stride.csv'\n",
        "    data = pd.read_csv(data_source)\n",
        "\n",
        "    start_preprocessing_time = time.time()\n",
        "    preprocessor = DataPreprocessor(data)\n",
        "    target_column = 'Reached.on.Time_Y.N'\n",
        "    preprocessor.perform_preprocessing(target_column)\n",
        "    end_preprocessing_time = time.time()\n",
        "\n",
        "    X_train = preprocessor.X_train\n",
        "    X_test = preprocessor.X_test\n",
        "    y_train = preprocessor.y_train\n",
        "    y_test = preprocessor.y_test\n",
        "\n",
        "    start_trainer_time = time.time()\n",
        "    trainer = ModelTrainer(X_train, X_test, y_train, y_test)\n",
        "    trainer.train_models()\n",
        "    end_trainer_time = time.time()\n",
        "    trainer.evaluate_models()\n",
        "\n",
        "    start_hyperparam_trainer_time = time.time()\n",
        "    trainer_with_hyperparam_tuning = ModelTrainerWithHyperparamTuning(X_train, X_test, y_train, y_test)\n",
        "    trainer_with_hyperparam_tuning.train_models_with_hyperparam_tuning()\n",
        "    end_hyperparam_trainer_time = time.time()\n",
        "\n",
        "    start_predict_time = time.time()\n",
        "    predictions = trainer.predict()\n",
        "    end_predict_time = time.time()\n",
        "\n",
        "    total_time = end_predict_time - start_total_time\n",
        "    preprocessing_time = end_preprocessing_time - start_preprocessing_time\n",
        "    trainer_time = end_trainer_time - start_trainer_time\n",
        "    hyperparam_trainer_time = end_hyperparam_trainer_time - start_hyperparam_trainer_time\n",
        "    predict_time = end_predict_time - start_predict_time\n",
        "\n",
        "    print(f\"Total execution time: {total_time} seconds\")\n",
        "    print(f\"Preprocessing time: {preprocessing_time} seconds\")\n",
        "    print(f\"Trainer time: {trainer_time} seconds\")\n",
        "    print(f\"Hyperparameter trainer time: {hyperparam_trainer_time} seconds\")\n",
        "    print(f\"Predict time: {predict_time} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwnFg5ErqHLakYtHfhDAEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}